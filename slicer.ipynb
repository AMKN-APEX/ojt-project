{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7521fc24",
   "metadata": {},
   "source": [
    "# 使い方\n",
    "これはmatファイルをptファイルに変換するノートブックです。<br>\n",
    "基本的には上から順に一つずつ実行していくことで、当該ファイルの変換がされます。<br>\n",
    "自分が設定しなければならないのは\n",
    "1. --- 定数の設定 ---\n",
    "2. --- keyの設定 --- \n",
    "\n",
    "--- 定数の設定 ---は、主にファイルのPATH指定になります。<br>\n",
    "--- keyの設定 ---は、matファイルに保存されたキーを--- keyの取得 ---で確認し、設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75538e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/onion/Documents/processed_data/train が作成されました。\n",
      "/mnt/c/Users/onion/Documents/processed_data/val が作成されました。\n",
      "/mnt/c/Users/onion/Documents/processed_data/test が作成されました。\n",
      "/mnt/c/Users/onion/Documents/processed_data/target が作成されました。\n",
      "[63000, 13500, 13500]\n"
     ]
    }
   ],
   "source": [
    "# --- 定数の設定 ---\n",
    "# .matのデータフォルダ場所\n",
    "MAT_PATH = \"/mnt/c/Users/onion/Documents/data\"\n",
    "\n",
    "# .ptのデータフォルダ保存場所\n",
    "PT_PATH = \"/mnt/c/Users/onion/Documents/processed_data\"\n",
    "\n",
    "# processed_dataのフォルダ構成\n",
    "PT_FOLDERS = [\"train\", \"val\", \"test\", \"target\"]\n",
    "\n",
    "# pathのディレクトリ部分が存在しない場合は作成、存在してもエラーにしない\n",
    "for PT_FOLDER in PT_FOLDERS:\n",
    "    os.makedirs(os.path.join(PT_PATH, PT_FOLDER), exist_ok=True)\n",
    "    print(f\"{os.path.join(PT_PATH, PT_FOLDER)} が作成されました。\")\n",
    "\n",
    "# targetのデータ名\n",
    "TARGET_NAMES = [\"m_train\", \"m_val\", \"m_test\", \"kappa_train\", \"kappa_val\", \"kappa_test\"]\n",
    "\n",
    "# .matのstructureデータフォルダ場所\n",
    "STRUCTURE_MAT_PATHS = [os.path.join(MAT_PATH, \"structures_train\"),\n",
    "                      os.path.join(MAT_PATH, \"structures_val\"),\n",
    "                      os.path.join(MAT_PATH, \"structures_test\")]\n",
    "\n",
    "# .matのstructureファイル数\n",
    "STRUCTURE_MAT_NUMS = [len(os.listdir(STRUCTURE_MAT_PATHS[0])),\n",
    "                      len(os.listdir(STRUCTURE_MAT_PATHS[1])),\n",
    "                      len(os.listdir(STRUCTURE_MAT_PATHS[2]))]\n",
    "print(STRUCTURE_MAT_NUMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a6d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_train.mat のkeyは ['y'] が設定されているよ\n",
      "m_val.mat のkeyは ['y'] が設定されているよ\n",
      "m_test.mat のkeyは ['y'] が設定されているよ\n",
      "kappa_train.mat のkeyは ['y'] が設定されているよ\n",
      "kappa_val.mat のkeyは ['y'] が設定されているよ\n",
      "kappa_test.mat のkeyは ['y'] が設定されているよ\n",
      "structure_0.mat のkeyは ['__header__', '__version__', '__globals__', 'BW'] が設定されているよ\n",
      "structure_1.mat のkeyは ['__header__', '__version__', '__globals__', 'BW'] が設定されているよ\n",
      "structure_2.mat のkeyは ['__header__', '__version__', '__globals__', 'BW'] が設定されているよ\n"
     ]
    }
   ],
   "source": [
    "# --- keyの取得 ---\n",
    "# targetのkeysを取得\n",
    "for TARGET_NAME in TARGET_NAMES:\n",
    "    target_mat_path = os.path.join(MAT_PATH, f\"{TARGET_NAME}.mat\")\n",
    "\n",
    "    with h5py.File(target_mat_path, \"r\") as f:\n",
    "        keys = list(f.keys())\n",
    "        print(f\"{TARGET_NAME}.mat のkeyは {keys} が設定されているよ\")\n",
    "\n",
    "# 初めの3つのstructureのkeysを取得\n",
    "for i in range(3):\n",
    "    train_structure_mat_path = os.path.join(STRUCTURE_MAT_PATHS[0], f\"structure_{i+1}.mat\")\n",
    "    data = sio.loadmat(train_structure_mat_path)\n",
    "    keys = list(data.keys())\n",
    "    print(f\"structure_{i}.mat のkeyは {keys} が設定されているよ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- keyの設定 ---\n",
    "# 上記のコードから取得したキーからほしいデータだけを指定する\n",
    "# ただし、キーがすべて同じである前提である\n",
    "TARGET_KEY = 'y'\n",
    "STRUCTURE_KEY = 'BW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe3459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_train.pt が作成されました。\n",
      "m_val.pt が作成されました。\n",
      "m_test.pt が作成されました。\n",
      "kappa_train.pt が作成されました。\n",
      "kappa_val.pt が作成されました。\n",
      "kappa_test.pt が作成されました。\n"
     ]
    }
   ],
   "source": [
    "# --- targetのmat2pt ---\n",
    "for TARGET_NAME in TARGET_NAMES:\n",
    "    target_mat_path = os.path.join(MAT_PATH, f\"{TARGET_NAME}.mat\")\n",
    "    target_pt_path = os.path.join(PT_PATH, PT_FOLDERS[3], f\"{TARGET_NAME}.pt\")\n",
    "\n",
    "    # MATLABファイルを読み込み、指定された変数をPyTorchのテンソルに変換して保存\n",
    "    with h5py.File(target_mat_path, \"r\") as f:\n",
    "        data = f[TARGET_KEY]\n",
    "        data = np.array(data).T\n",
    "        data = torch.tensor(data)\n",
    "        torch.save(data, target_pt_path)\n",
    "\n",
    "        if os.path.exists(target_pt_path):\n",
    "            print(f\"{TARGET_NAME}.pt が作成されました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b32b5d05",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:659] . unexpected pos 704 vs 598",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ojt-project/.venv/lib/python3.12/site-packages/torch/serialization.py:965\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ojt-project/.venv/lib/python3.12/site-packages/torch/serialization.py:1266\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1265\u001b[39m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1266\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at inline_container.cc:857] . PytorchStreamWriter failed writing file data/0: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m data = np.array(data).T\n\u001b[32m     10\u001b[39m data = torch.tensor(data)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructure_pt_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ojt-project/.venv/lib/python3.12/site-packages/torch/serialization.py:964\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    961\u001b[39m     f = os.fspath(f)\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m    970\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ojt-project/.venv/lib/python3.12/site-packages/torch/serialization.py:798\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__exit__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m798\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    800\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_stream.close()\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at inline_container.cc:659] . unexpected pos 704 vs 598"
     ]
    }
   ],
   "source": [
    "# --- structureのmat2pt ---\n",
    "for i in range(3): # [train, val, test]の3つ\n",
    "    for j in range(STRUCTURE_MAT_NUMS[i]):\n",
    "        structure_mat_path = os.path.join(STRUCTURE_MAT_PATHS[i], f\"structure_{j+1}.mat\")\n",
    "        structure_pt_path = os.path.join(PT_PATH, PT_FOLDERS[i], f\"structure_{j+1}.pt\")\n",
    "\n",
    "        data = sio.loadmat(structure_mat_path)\n",
    "        data = data[STRUCTURE_KEY]\n",
    "        data = np.array(data).T\n",
    "        data = torch.tensor(data)\n",
    "        torch.save(data, structure_pt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
